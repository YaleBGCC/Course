<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Cory Merow" />


<title>Introduction to Species Distribution Modeling</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="cm_yeti_bootswatch.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 66px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 71px;
  margin-top: -71px;
}

.section h2 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h3 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h4 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h5 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h6 {
  padding-top: 71px;
  margin-top: -71px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "show");
});
</script>




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Ecology, Statistics, and Data Science with R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="Head2_Schedule.html">Schedule</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Course Materials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Head1_CourseContent.html">About Course Content</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Module 1: Introduction to R</li>
    <li>
      <a href="00_CourseIntroductionFrame.html">1.0 Course Introduction</a>
    </li>
    <li>
      <a href="01_Rintro.html">1.1 First Steps</a>
    </li>
    <li>
      <a href="02_DataWrangling.html">1.2 Data Wrangling</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Module 2: Spatial Analysis in R</li>
    <li>
      <a href="05_Raster.html">2.1 Spatial Data</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Module 3: Species Distribution Models</li>
    <li>
      <a href="101SDMs.html">3.1 Basic SDMS</a>
    </li>
    <li class="divider"></li>
  </ul>
</li>
<li>
  <a href="Head3_Resources.html">Resources</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/cmerow/RDataScience/tree/gh-pages">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Introduction to Species Distribution Modeling</h1>
<h4 class="author"><em><a href="cmerow.github.io">Cory Merow</a></em></h4>

</div>


<!-- <div> -->
<!-- <iframe src="05_presentation/05_Spatial.html" width="100%" height="700px"> </iframe> -->
<!-- </div> -->
<p><a href="101SDMs.R"><i class="fa fa-file-code-o fa-3x" aria-hidden="true"></i> The R Script associated with this page is available here</a>. Download this file and open it (or copy-paste into a new script) with RStudio so you can follow along.</p>
<div id="setup" class="section level1">
<h1><span class="header-section-number">1</span> Setup</h1>
<pre class="r"><code>library(spocc)
library(raster)</code></pre>
<pre><code>## Loading required package: sp</code></pre>
<pre class="r"><code>library(sp)
library(rgdal)</code></pre>
<pre><code>## rgdal: version: 1.2-16, (SVN revision 701)
##  Geospatial Data Abstraction Library extensions to R successfully loaded
##  Loaded GDAL runtime: GDAL 2.1.3, released 2017/20/01
##  Path to GDAL shared files: /Library/Frameworks/R.framework/Versions/3.4/Resources/library/rgdal/gdal
##  GDAL binary built with GEOS: FALSE 
##  Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493]
##  Path to PROJ.4 shared files: /Library/Frameworks/R.framework/Versions/3.4/Resources/library/rgdal/proj
##  Linking to sp version: 1.2-5</code></pre>
<pre class="r"><code>library(ROCR)</code></pre>
<pre><code>## Loading required package: gplots</code></pre>
<pre><code>## 
## Attaching package: &#39;gplots&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     lowess</code></pre>
<pre class="r"><code>library(corrplot)</code></pre>
<pre><code>## corrplot 0.84 loaded</code></pre>
<pre class="r"><code>library(maxnet)
library(spThin)</code></pre>
<pre><code>## Loading required package: spam</code></pre>
<pre><code>## Loading required package: grid</code></pre>
<pre><code>## Spam version 1.4-0 (2016-08-29) is loaded.
## Type &#39;help( Spam)&#39; or &#39;demo( spam)&#39; for a short introduction 
## and overview of this package.
## Help for individual functions is also obtained by adding the
## suffix &#39;.spam&#39; to the function name, e.g. &#39;help( chol.spam)&#39;.</code></pre>
<pre><code>## 
## Attaching package: &#39;spam&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     backsolve, forwardsolve</code></pre>
<pre><code>## Loading required package: fields</code></pre>
<pre><code>## Loading required package: maps</code></pre>
<pre><code>## Loading required package: knitr</code></pre>
</div>
<div id="the-worst-sdm-ever" class="section level1">
<h1><span class="header-section-number">2</span> The worst SDM ever</h1>
<p>The goal of this section is to use the simplest possible set of operations to build an SDM. There are many packages that will perform much more refined versions of these steps, at the expense that decisions are made behind the scenes, or may be obscure to the user. So before getting into fancier tools, let’s see what the bare minimum looks like.</p>
<blockquote>
<p>This is not the simplest possible code, because it requires some familiarity with the internal components of different spatial objects. The tradeoff is that none of the key operations are performed behind the scenes by specialized SDM functions. I realize this is not always pretty, but I hope for that reason it can demonstrate some coding gynmastics for beginners.</p>
</blockquote>
<div id="get-presence-data" class="section level2">
<h2><span class="header-section-number">2.1</span> Get presence data</h2>
<p>The <code>spocc</code> package allows you to hit a number of the larger databases for presence-only data within R. They provide a number of useful pieces of metadata, if your’e into that sort of this. For this, we’re not; we just want lat and lon.</p>
<blockquote>
<p>Decision: You assume the database of choice has sufficiently checked for errors in biology or typos. You know what happens when you assume…</p>
</blockquote>
<pre class="r"><code># get presence data
# pres=spocc::occ(&#39;Alliaria petiolata&#39;,from=&#39;gbif&#39;,limit=5000) # this can be slow
  # so just read in the result of me running this earlier
pres=read.csv(&#39;https://cmerow.github.io/YaleBGCCourses/101_assets/AP_gbif.csv&#39;)[,c(&#39;longitude&#39;,&#39;latitude&#39;)]
pres=pres[complete.cases(pres),] # toss records without coords</code></pre>
</div>
<div id="get-environmental-data" class="section level2">
<h2><span class="header-section-number">2.2</span> Get environmental data</h2>
<p>The <code>raster</code> package has a convenience function to get some types of data. To see more about <a href="http://worldclim.org/version2">Worldclim</a></p>
<blockquote>
<p>Decision: Worldclim data describes the environmental well in this region. The ‘bioclim’ variables are biologically relevant summaries of climate.</p>
</blockquote>
<pre class="r"><code># get climate data
  # the raster package has convenience function built in for worldclim
clim=getData(&#39;worldclim&#39;, var=&#39;bio&#39;, res=10)</code></pre>
<p>The Bioclim variables in <code>clim.us</code> are:</p>
<p><small></p>
<table>
<thead>
<tr class="header">
<th>Varia</th>
<th>ble Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BIO1</td>
<td>Annual Mean Temperature</td>
</tr>
<tr class="even">
<td>BIO2</td>
<td>Mean Diurnal Range (Mean of monthly (max temp – min temp))</td>
</tr>
<tr class="odd">
<td>BIO3</td>
<td>Isothermality (BIO2/BIO7) (* 100)</td>
</tr>
<tr class="even">
<td>BIO4</td>
<td>Temperature Seasonality (standard deviation *100)</td>
</tr>
<tr class="odd">
<td>BIO5</td>
<td>Max Temperature of Warmest Month</td>
</tr>
<tr class="even">
<td>BIO6</td>
<td>Min Temperature of Coldest Month</td>
</tr>
<tr class="odd">
<td>BIO7</td>
<td>Temperature Annual Range (BIO5-BIO6)</td>
</tr>
<tr class="even">
<td>BIO8</td>
<td>Mean Temperature of Wettest Quarter</td>
</tr>
<tr class="odd">
<td>BIO9</td>
<td>Mean Temperature of Driest Quarter</td>
</tr>
<tr class="even">
<td>BIO10</td>
<td>Mean Temperature of Warmest Quarter</td>
</tr>
<tr class="odd">
<td>BIO11</td>
<td>Mean Temperature of Coldest Quarter</td>
</tr>
<tr class="even">
<td>BIO12</td>
<td>Annual Precipitation</td>
</tr>
<tr class="odd">
<td>BIO13</td>
<td>Precipitation of Wettest Month</td>
</tr>
<tr class="even">
<td>BIO14</td>
<td>Precipitation of Driest Month</td>
</tr>
<tr class="odd">
<td>BIO15</td>
<td>Precipitation Seasonality (Coefficient of Variation)</td>
</tr>
<tr class="even">
<td>BIO16</td>
<td>Precipitation of Wettest Quarter</td>
</tr>
<tr class="odd">
<td>BIO17</td>
<td>Precipitation of Driest Quarter</td>
</tr>
<tr class="even">
<td>BIO18</td>
<td>Precipitation of Warmest Quarter</td>
</tr>
<tr class="odd">
<td>BIO19</td>
<td>Precipitation of Coldest Quarter</td>
</tr>
</tbody>
</table>
<p></small></p>
</div>
<div id="choose-domain" class="section level2">
<h2><span class="header-section-number">2.3</span> Choose domain</h2>
<p>The ‘domain’ is the region of interest. It can be a political region, a biome, a park, a watershed, etc. It should include locations where the species is present and absent. Choosing relevent locations were the species does not occur is part of the art of <em>presence-only</em> modeling (see slides above).</p>
<blockquote>
<p>Decision: We are only asking about invasion in New England, so we constrain the domain to a bounding box around New England</p>
</blockquote>
<pre class="r"><code># choose domain (just the Eastern US)
clim.us=raster::crop(clim,c(-76,-65,40,50)) # trim to a smaller region
plot(clim.us[[1]]) # plot just the 1st variable to see domain</code></pre>
<p><img src="101SDMs_files/figure-html/unnamed-chunk-5-1.png" /><!-- --></p>
</div>
<div id="prep-data" class="section level2">
<h2><span class="header-section-number">2.4</span> Prep data</h2>
<p>Many climate variables are highly correlated with one another, which can confound statistical analyses.</p>
<blockquote>
<p>Decision: Correlated predictors can make it difficult to interpret model coefficients or response curves. So we’ll remove the most correlated predictores</p>
</blockquote>
<pre class="r"><code># check for correlated predictors
cors=cor(values(clim.us),use=&#39;complete.obs&#39;) # evaluate correlations
corrplot(cors,order = &quot;AOE&quot;, addCoef.col = &quot;grey&quot;,number.cex=.6) # plot correlations</code></pre>
<p><img src="101SDMs_files/figure-html/unnamed-chunk-6-1.png" /><!-- --></p>
<p>This plot nicely clumps groups of similar variables. Choose a representative variable from each clump.</p>
<pre class="r"><code>clim=clim[[c(&quot;bio1&quot;,&quot;bio2&quot;,&quot;bio13&quot;,&quot;bio14&quot;)]] # keep just reasonably uncorrelated ones
clim.us=clim.us[[c(&#39;bio1&#39;,&#39;bio2&#39;,&#39;bio13&#39;,&#39;bio14&#39;)]] # keep just reasonably uncorrelated ones
cors=cor(values(clim.us),use=&#39;complete.obs&#39;) # evaluate correlations
corrplot(cors,order = &quot;AOE&quot;, addCoef.col = &quot;grey&quot;,number.cex=.6)# plot correlations</code></pre>
<p><img src="101SDMs_files/figure-html/unnamed-chunk-7-1.png" /><!-- --></p>
<p>Ok, tolerable. Some people advocate that correlations should be &lt;0.7. I prefer lower, like 0.3, or 0.4 because I often forecast (as we’ll do below) and one must assume that those correlations hold in new scenarios hold to make meaningful forecasts.</p>
<p>Scaling each predictor to zero mean and unit variance is a common statistical approach to make sure the coefficents you’ll estimate are comparable (on the same scale) and prevents a few other wonky things from possibly happening.</p>
<pre class="r"><code># scale each predictor to mean=0, variance=1
clim.means=apply(values(clim.us),2,mean,na.rm=T) # means
clim.sds=apply(values(clim.us),2,sd,na.rm=T) # standard devations
name=names(clim.us)
values(clim.us)=sapply(1:nlayers(clim.us),function(x) (values(clim.us)[,x]-clim.means[x])/clim.sds[x]) 
# z-scores
names(clim.us)=name

# get environment at pres points
coordinates(pres)=c(&#39;longitude&#39;,&#39;latitude&#39;) # set coords to allow extraction (next line)
pres.data=data.frame(raster::extract(clim.us,pres)) # extract data at pres locations
coordinates(pres.data)=coordinates(pres) # make sure the data have coords associated
pres.data=pres.data[complete.cases(pres.data@data),] # toss points without env data</code></pre>
<pre class="r"><code>plot(clim.us) # view </code></pre>
<p><img src="101SDMs_files/figure-html/unnamed-chunk-9-1.png" /><!-- --></p>
</div>
<div id="sample-background" class="section level2">
<h2><span class="header-section-number">2.5</span> Sample background</h2>
<p>In presence-only (PO) modeling, where absence data do not exist, so-called ‘background’ (==jargon) points are used. In PO models, one compares the environmental conditions at occupied locations (presences) to the conditions available in the region of interest. This corresponds to ‘use-avialability’ (==jargon) analysis and asks, ‘how much does the species use environment x in proportion to its availability?’ For example, if a landscape contains 10 cells with temperature &lt; 20 degrees, and the species uses all of them, you would infer that cold locations are important (10 are used and 10 are available). In contrast, if the lanscape had 1000 cells with temperature &lt;20 degrees, you’d infer the opposite, that cold cells are probably avoided (10 are used and 1000 are available). This is the essence of (this type of) presence-only modeling.</p>
<blockquote>
<p>Decision: The species is equally likely to be anywhere on the landscapes, so we’ll compare presences to a random sample of background points.</p>
</blockquote>
<p>(There’s a lot of subtlties about background selection, just go with it for now…)</p>
<pre class="r"><code>    ## save the data table
# sample background (to compare against presences)
all.background=which(complete.cases(values(clim.us))) # find cells on land
bg.index=sample(all.background,min(length(all.background),10000)) # take random sample of land
bg.data=data.frame(values(clim.us)[bg.index,]) # get the env at these cells
coordinates(bg.data)=coordinates(clim.us)[bg.index,] # define spatial object</code></pre>
</div>
<div id="statistical-model" class="section level2">
<h2><span class="header-section-number">2.6</span> Statistical model</h2>
<blockquote>
<p>Decision: Linear and quadratic terms are sufficient to describe the species’ response to the environment.</p>
</blockquote>
<p>Next, combine the data into a convenient form and specify a formula for the regression.</p>
<pre class="r"><code># prep data for use in glm()
all.data=rbind(data.frame(pres=1,pres.data@data),data.frame(pres=0,bg.data@data)) # line up pres &amp; bg

# specify formula (quickly to avoid writing out every name)
(form=paste(&#39;pres/weight~&#39;, # lhs of eqn.
            paste(names(all.data)[-1], collapse = &quot; + &quot;),&#39;+&#39;, # linear terms
            paste(&quot;I(&quot;, names(all.data)[-1], &quot;^2)&quot;, sep = &quot;&quot;, collapse = &quot; + &quot;))) # qudratic terms</code></pre>
<pre><code>## [1] &quot;pres/weight~ bio1 + bio2 + bio13 + bio14 + I(bio1^2) + I(bio2^2) + I(bio13^2) + I(bio14^2)&quot;</code></pre>
<p>There are some subtle differences here compared to a regular old GLM. These weights allow one to fit a Poisson point process model with the <code>glm</code> function. If this sort of thing excites you, <a href="http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12352/abstract">this paper</a> describes point process models well, and the appendix describes this weighting scheme. If it doesn’t, just pretend this is a regular GLM for now.</p>
<pre class="r"><code>all.data$weight = all.data$pres + (1 - all.data$pres) * 10000 # these allow you to fit a Point Process
mod.worst=glm(form,data=all.data,family=poisson(link=&#39;log&#39;),weights=weight) # fit the model
summary(mod.worst) # show coefficients</code></pre>
<pre><code>## 
## Call:
## glm(formula = form, family = poisson(link = &quot;log&quot;), data = all.data, 
##     weights = weight)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -4.0280  -0.2408  -0.0706  -0.0308   5.1147  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -15.37924    0.30294 -50.766  &lt; 2e-16 ***
## bio1          2.58714    0.30915   8.369  &lt; 2e-16 ***
## bio2          0.88788    0.07252  12.243  &lt; 2e-16 ***
## bio13         0.32046    0.09170   3.495 0.000475 ***
## bio14        -0.96095    0.09545 -10.067  &lt; 2e-16 ***
## I(bio1^2)     0.21355    0.10382   2.057 0.039705 *  
## I(bio2^2)     0.23243    0.03706   6.272 3.57e-10 ***
## I(bio13^2)    0.22246    0.06308   3.526 0.000421 ***
## I(bio14^2)    0.41497    0.05867   7.073 1.52e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 8340.1  on 3353  degrees of freedom
## Residual deviance: 6823.3  on 3345  degrees of freedom
## AIC: 7579.3
## 
## Number of Fisher Scoring iterations: 17</code></pre>
</div>
<div id="inspect-response-curves" class="section level2">
<h2><span class="header-section-number">2.7</span> Inspect response curves</h2>
<p>Response curves describe how the species’ occurrence (y-axis) depends on a single climate variable (x-axis). There’s one for each environmental variable in the model. Usually this is done by making predictions with all the other predictors set at their means. Most packages have 1-liners to make this instead, but this builds more character.</p>
<pre class="r"><code># check response curves
  # these marginal response curves are evaluated at the means of the non-focal predictor
clim.ranges=apply(values(clim.us),2,range,na.rm=T) # upper and lower limits for each variable
dummy.mean.matrix=data.frame(matrix(0,ncol=nlayers(clim.us),nrow=100)) #makes prediction concise below
names(dummy.mean.matrix)=colnames(clim.ranges) # line up names for later reference
response.curves=lapply(1:nlayers(clim.us),function(x){ # loop over each variable
  xs=seq(clim.ranges[1,x],clim.ranges[2,x],length=100) # x values to evaluate the curve
  newdata=dummy.mean.matrix # data frame with right structure
  newdata[,x]=xs # plug in just the values for the focal variable that differ from mean
  ys=predict(mod.worst,newdata=newdata) # predictions
  return(data.frame(xs=xs,ys=ys)) # define outputs
})# ignore warnings</code></pre>
<p>Check out the list of lists that store this.</p>
<pre class="r"><code>str(response.curves) #structure of the object used for plotting</code></pre>
<pre><code>## List of 4
##  $ :&#39;data.frame&#39;:    100 obs. of  2 variables:
##   ..$ xs: num [1:100] -1.79 -1.74 -1.7 -1.65 -1.61 ...
##   ..$ ys: num [1:100] -19.3 -19.2 -19.2 -19.1 -19 ...
##  $ :&#39;data.frame&#39;:    100 obs. of  2 variables:
##   ..$ xs: num [1:100] -4.75 -4.68 -4.61 -4.54 -4.47 ...
##   ..$ ys: num [1:100] -14.3 -14.4 -14.5 -14.6 -14.7 ...
##  $ :&#39;data.frame&#39;:    100 obs. of  2 variables:
##   ..$ xs: num [1:100] -2.82 -2.76 -2.7 -2.64 -2.58 ...
##   ..$ ys: num [1:100] -14.5 -14.6 -14.6 -14.7 -14.7 ...
##  $ :&#39;data.frame&#39;:    100 obs. of  2 variables:
##   ..$ xs: num [1:100] -2.52 -2.46 -2.4 -2.34 -2.28 ...
##   ..$ ys: num [1:100] -10.3 -10.5 -10.7 -10.9 -11 ...</code></pre>
<pre class="r"><code>  # plot the curves
par(mfrow=c(2,2),mar=c(4,5,.5,.5)) # # rows and cols for plotting
for(i in 1:nlayers(clim.us)){ # loop over layers
  plot(response.curves[[i]]$xs,response.curves[[i]]$ys, # xs and ys
       type=&#39;l&#39;, # line plot
       bty=&#39;n&#39;,las=1, # decorations
       ylim=c(-20,20), # y axis limits
       xlab=colnames(clim.ranges)[i],ylab=&#39;occurence rate&#39;) # axis labels
  pres.env.range=range(pres.data[names(clim.us)[i]]@data)  # find limits of fitting data
  abline(v=pres.env.range,col=&#39;red&#39;,lty=2)  # plot limits of fitting data
}</code></pre>
<p><img src="101SDMs_files/figure-html/unnamed-chunk-15-1.png" /><!-- --></p>
</div>
<div id="map-predictions" class="section level2">
<h2><span class="header-section-number">2.8</span> Map predictions</h2>
<blockquote>
<p>Decision: When predicting, its ok to extrapolate beyond</p>
</blockquote>
<pre class="r"><code># predict to US
pred.r=raster::predict(clim.us,mod.worst, index=1,type=&quot;response&quot;)
pred.r=pred.r/sum(values(pred.r),na.rm=T) # normalize prediction (sum to 1)
plot(log(pred.r)) # plot raster
plot(pres,add=T) # plot points</code></pre>
<p><img src="101SDMs_files/figure-html/unnamed-chunk-16-1.png" /><!-- --></p>
</div>
<div id="evaluate-performance" class="section level2">
<h2><span class="header-section-number">2.9</span> Evaluate performance</h2>
<p>Reciever-operator characteristic (ROC) curves are often used to evaluation binary (presence/absence) predictions. Since the predictions are continuous (see previous map), we need to choose a threshold that distinguishes presence from absence. The ROC curve summarizes the results for all possible thresholds. Each point corresponds to a threshold, the y-axis describes the proportion of presences correctly predicted while the x-axis describes the proportion of background points where presence is predicted. There’s a clear tradeoff in getting a lot of presences right without predicting presence everywhere. The AUC (area under the curve) describes the area under the ROC curve - a value of 1 is the best, and a value of 0.5 means you may as well flip a coin. More <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">here</a>.</p>
<pre class="r"><code># evaluate
pred.at.fitting.pres=raster::extract(pred.r,pres.data) # get predictions at pres locations
pred.at.fitting.bg=raster::extract(pred.r,bg.data) # get predictions at background locations
rocr.pred=ROCR::prediction(predictions=c(pred.at.fitting.pres,pred.at.fitting.bg),
                          labels=c(rep(1,length(pred.at.fitting.pres)),rep(0,length(pred.at.fitting.bg)))) # define the prediction object needed by ROCR
perf.fit=performance(rocr.pred,measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) # calculate perfomance 
plot(perf.fit) # plot ROC curve
abline(0,1) # 1:1 line indicate random predictions </code></pre>
<p><img src="101SDMs_files/figure-html/unnamed-chunk-17-1.png" /><!-- --></p>
<pre class="r"><code>(auc_ROCR &lt;- performance(rocr.pred, measure = &quot;auc&quot;)@y.values[[1]]) # get AUC</code></pre>
<pre><code>## [1] 0.9412932</code></pre>
</div>
<div id="transfer-to-new-conditions" class="section level2">
<h2><span class="header-section-number">2.10</span> Transfer to new conditions</h2>
<p>A common goal of SDMing is to transfer the models to new locations.</p>
<blockquote>
<p>Decision: The occurrence-environment relationship fit in New England also describes the species response to environment in Europe.</p>
</blockquote>
<pre class="r"><code># transfer to Europe
# choose domain (just europe)
clim.eu=raster::crop(clim,c(-10,55,30,75))
values(clim.eu)=sapply(1:nlayers(clim.eu),function(x) (values(clim.eu)[,x]-clim.means[x])/clim.sds[x])
names(clim.eu)=names(clim.us)
# z-scores (to make values comparable to the scaeld values for fitting)
transfer.r=raster::predict(clim.eu,mod.worst, index=1,type=&quot;response&quot;)
transfer.r=transfer.r/sum(values(transfer.r),na.rm=T) # normalize prediction (sum to 1)
plot(log(transfer.r)) # plot preds
plot(pres,add=T) # plot presences </code></pre>
<p><img src="101SDMs_files/figure-html/unnamed-chunk-18-1.png" /><!-- --></p>
</div>
</div>
<div id="improvements" class="section level1">
<h1><span class="header-section-number">3</span> Improvements</h1>
<!-- #========================================================================= -->
<div id="sampling-bias" class="section level2">
<h2><span class="header-section-number">3.1</span> Sampling bias</h2>
<div id="sample-background-1" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Sample background</h3>
<blockquote>
<p>Decision: Presences are most likelt to be observed where other specie sampled with the same protocol or are taxonomically similar were sampled.</p>
</blockquote>
<p>The data in <code>bias.bg</code> are the result of extracting the coordinates of all 187 species observed in the Invasive Plant Atlas of New England (IPANE) data base.</p>
<pre class="r"><code>bias.bg=read.csv(&#39;/Users/ctg/Dropbox/Projects/Workshops/YaleBGCCourses/101_assets/Bias_IPANE_allPoints.csv&#39;)[,-1]
coordinates(bias.bg)=c(1,2)
bias.bg.data=data.frame(raster::extract(clim.us,bias.bg))
coordinates(bias.bg.data)=coordinates(bias.bg)
bias.bg.data=bias.bg.data[complete.cases(bias.bg.data@data),]</code></pre>
<pre class="r"><code># prep data for use in glm()
all.data=rbind(data.frame(pres=1,pres.data@data),data.frame(pres=0,bias.bg.data@data))

# specify formula (quickly to avoid writing out every name)
(form=paste(&#39;pres/weight~&#39;, # lhs of eqn.
            paste(names(all.data)[-1], collapse = &quot; + &quot;),&#39;+&#39;, # linear terms
            paste(&quot;I(&quot;, names(all.data)[-1], &quot;^2)&quot;, sep = &quot;&quot;, collapse = &quot; + &quot;))) # qudratic terms</code></pre>
<pre><code>## [1] &quot;pres/weight~ bio1 + bio2 + bio13 + bio14 + I(bio1^2) + I(bio2^2) + I(bio13^2) + I(bio14^2)&quot;</code></pre>
<p><em>From this point on, the code is exactly the same as the previous example, except that the model is given a new name, <code>mod.bias</code> instead of <code>mod.worst</code></em></p>
</div>
</div>
<div id="statistical-model-1" class="section level2">
<h2><span class="header-section-number">3.2</span> Statistical model</h2>
<pre class="r"><code>all.data$weight = all.data$pres + (1 - all.data$pres) * 10000 # these allow you to fit a Point Process
mod.bias=glm(form,data=all.data,family=poisson(link=&#39;log&#39;),weights=weight) # fit the model
summary(mod.bias) # show coefficients</code></pre>
<pre><code>## 
## Call:
## glm(formula = form, family = poisson(link = &quot;log&quot;), data = all.data, 
##     weights = weight)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7729  -0.2560  -0.1099  -0.0320   6.3131  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -17.31494    0.28372 -61.028  &lt; 2e-16 ***
## bio1         -5.04764    0.31736 -15.905  &lt; 2e-16 ***
## bio2          2.59214    0.07989  32.446  &lt; 2e-16 ***
## bio13         1.15896    0.09228  12.560  &lt; 2e-16 ***
## bio14        -2.61444    0.07388 -35.389  &lt; 2e-16 ***
## I(bio1^2)     5.08107    0.12202  41.641  &lt; 2e-16 ***
## I(bio2^2)     0.66814    0.05442  12.277  &lt; 2e-16 ***
## I(bio13^2)    0.79904    0.10116   7.899 2.81e-15 ***
## I(bio14^2)   -0.08801    0.08161  -1.078    0.281    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 8805.4  on 5976  degrees of freedom
## Residual deviance: 5167.8  on 5968  degrees of freedom
## AIC: 5923.8
## 
## Number of Fisher Scoring iterations: 18</code></pre>
</div>
<div id="inspect-response-curves-1" class="section level2">
<h2><span class="header-section-number">3.3</span> Inspect response curves</h2>
<pre class="r"><code># check response curves
  # these marginal response curves are evaluated at the means of the non-focal predictor
clim.ranges=apply(values(clim.us),2,range,na.rm=T) # upper and lower limits for each variable
dummy.mean.matrix=data.frame(matrix(0,ncol=nlayers(clim.us),nrow=100)) #makes prediction concise below
names(dummy.mean.matrix)=colnames(clim.ranges) # line up names for later reference
response.curves=lapply(1:nlayers(clim.us),function(x){ # loop over each variable
  xs=seq(clim.ranges[1,x],clim.ranges[2,x],length=100) # x values to evaluate the curve
  newdata=dummy.mean.matrix # data frame with right structure
  newdata[,x]=xs # plug in just the values for the focal variable that differ from mean
  ys=predict(mod.bias,newdata=newdata) # predictions
  return(data.frame(xs=xs,ys=ys)) # define outputs
})# ignore warnings</code></pre>
<pre class="r"><code>str(response.curves) #structure of the object used for plotting</code></pre>
<pre><code>## List of 4
##  $ :&#39;data.frame&#39;:    100 obs. of  2 variables:
##   ..$ xs: num [1:100] -1.79 -1.74 -1.7 -1.65 -1.61 ...
##   ..$ ys: num [1:100] 7.92 6.89 5.88 4.89 3.92 ...
##  $ :&#39;data.frame&#39;:    100 obs. of  2 variables:
##   ..$ xs: num [1:100] -4.75 -4.68 -4.61 -4.54 -4.47 ...
##   ..$ ys: num [1:100] -14.5 -14.8 -15.1 -15.3 -15.6 ...
##  $ :&#39;data.frame&#39;:    100 obs. of  2 variables:
##   ..$ xs: num [1:100] -2.82 -2.76 -2.7 -2.64 -2.58 ...
##   ..$ ys: num [1:100] -14.2 -14.4 -14.6 -14.8 -15 ...
##  $ :&#39;data.frame&#39;:    100 obs. of  2 variables:
##   ..$ xs: num [1:100] -2.52 -2.46 -2.4 -2.34 -2.28 ...
##   ..$ ys: num [1:100] -11.3 -11.4 -11.6 -11.7 -11.8 ...</code></pre>
<pre class="r"><code>  # plot the curves
par(mfrow=c(2,2),mar=c(4,5,.5,.5)) # # rows and cols for plotting
for(i in 1:nlayers(clim.us)){ # loop over layers
  plot(response.curves[[i]]$xs,response.curves[[i]]$ys,
       type=&#39;l&#39;,bty=&#39;n&#39;,las=1,xlab=colnames(clim.ranges)[i],ylab=&#39;occurence rate&#39;,ylim=c(-20,20))
  pres.env.range=range(pres.data[names(clim.us)[i]]@data) # find limits of fitting data
  abline(v=pres.env.range,col=&#39;red&#39;,lty=2) # plot limits of fitting data
}</code></pre>
<p><img src="101SDMs_files/figure-html/unnamed-chunk-24-1.png" /><!-- --></p>
<div id="map-predictions-1" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Map predictions</h3>
<pre class="r"><code># predict to US
pred.r=raster::predict(clim.us,mod.bias, index=1,type=&quot;response&quot;)
pred.r=pred.r/sum(values(pred.r),na.rm=T) # normalize prediction (sum to 1)
plot(log(pred.r)) # plot raster
plot(pres,add=T) # plot points</code></pre>
<p><img src="101SDMs_files/figure-html/unnamed-chunk-25-1.png" /><!-- --></p>
</div>
<div id="evaluate-performance-1" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Evaluate performance</h3>
<pre class="r"><code># evaluate
pred.at.fitting.pres=raster::extract(pred.r,pres.data) # get predictions at pres locations
pred.at.fitting.bg=raster::extract(pred.r,bg.data) # get predictions at background locations
rocr.pred=ROCR::prediction(predictions=c(pred.at.fitting.pres,pred.at.fitting.bg),
                          labels=c(rep(1,length(pred.at.fitting.pres)),rep(0,length(pred.at.fitting.bg)))) # define the prediction object needed by ROCR
perf.fit=performance(rocr.pred,measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) # calculate perfomance 
plot(perf.fit) # plot ROC curve
abline(0,1) # 1:1 line indicate random predictions </code></pre>
<p><img src="101SDMs_files/figure-html/unnamed-chunk-26-1.png" /><!-- --></p>
<pre class="r"><code>(auc_ROCR &lt;- performance(rocr.pred, measure = &quot;auc&quot;)@y.values[[1]]) # get AUC</code></pre>
<pre><code>## [1] 0.6737413</code></pre>
<p>Oh, Snap! Not nearly as good as the model that ignored bias. This is common when accounting for bias; you’ve removed something that structured the observations from the model. So you evaluation on the <em>fitting</em> data will be worse. Performance on new data sets will often be better (conditional on the rest of the model being well designed), so long as those don’t suffer from the same sampling bias patterns.</p>
<!-- #========================================================================= -->
<!-- #========================================================================= -->
</div>
</div>
<div id="other-algorithms-glmnet" class="section level2">
<h2><span class="header-section-number">3.4</span> Other algorithms: glmnet</h2>
</div>
<div id="statistical-model-2" class="section level2">
<h2><span class="header-section-number">3.5</span> Statistical model</h2>
<pre class="r"><code>mod.maxnet=maxnet(p=all.data[,&#39;pres&#39;],data=all.data[,c(&quot;bio1&quot;,&quot;bio2&quot;,&quot;bio13&quot;,&quot;bio14&quot;)])
summary(mod.maxnet) # show coefficients</code></pre>
<pre><code>##                Length Class     Mode     
## a0               200  -none-    numeric  
## beta           81200  dgCMatrix S4       
## df               200  -none-    numeric  
## dim                2  -none-    numeric  
## lambda           200  -none-    numeric  
## dev.ratio        200  -none-    numeric  
## nulldev            1  -none-    numeric  
## npasses            1  -none-    numeric  
## jerr               1  -none-    numeric  
## offset             1  -none-    logical  
## classnames         2  -none-    character
## call               8  -none-    call     
## nobs               1  -none-    numeric  
## betas             24  -none-    numeric  
## alpha              1  -none-    numeric  
## entropy            1  -none-    numeric  
## penalty.factor   406  -none-    numeric  
## featuremins      406  -none-    numeric  
## featuremaxs      406  -none-    numeric  
## varmin             4  -none-    numeric  
## varmax             4  -none-    numeric  
## samplemeans        4  -none-    list     
## levels             4  -none-    list</code></pre>
<p><em>From this point on, the code is exactly the same as the previous example, except that the model is given a new name, <code>mod.maxnet</code> instead of <code>mod.bias</code></em></p>
</div>
<div id="inspect-response-curves-2" class="section level2">
<h2><span class="header-section-number">3.6</span> Inspect response curves</h2>
<pre class="r"><code># check response curves
  # these marginal response curves are evaluated at the means of the non-focal predictor
clim.ranges=apply(values(clim.us),2,range,na.rm=T) # upper and lower limits for each variable
dummy.mean.matrix=data.frame(matrix(0,ncol=nlayers(clim.us),nrow=100)) #makes prediction concise below
names(dummy.mean.matrix)=colnames(clim.ranges) # line up names for later reference
response.curves=lapply(1:nlayers(clim.us),function(x){ # loop over each variable
  xs=seq(clim.ranges[1,x],clim.ranges[2,x],length=100) # x values to evaluate the curve
  newdata=dummy.mean.matrix # data frame with right structure
  newdata[,x]=xs # plug in just the values for the focal variable that differ from mean
  ys=predict(mod.maxnet,newdata=newdata) # predictions
  return(data.frame(xs=xs,ys=ys)) # define outputs
})# ignore warnings</code></pre>
<pre class="r"><code>  # plot the curves
par(mfrow=c(2,2),mar=c(4,5,.5,.5)) # # rows and cols for plotting
for(i in 1:nlayers(clim.us)){ # loop over layers
  plot(response.curves[[i]]$xs,response.curves[[i]]$ys,
       type=&#39;l&#39;,bty=&#39;n&#39;,las=1,xlab=colnames(clim.ranges)[i],ylab=&#39;occurence rate&#39;,ylim=c(-20,20))
  pres.env.range=range(pres.data[names(clim.us)[i]]@data)  # find limits of fitting data
  abline(v=pres.env.range,col=&#39;red&#39;,lty=2)  # plot limits of fitting data
}</code></pre>
<p><img src="101SDMs_files/figure-html/unnamed-chunk-29-1.png" /><!-- --></p>
<div id="map-predictions-2" class="section level3">
<h3><span class="header-section-number">3.6.1</span> Map predictions</h3>
<pre class="r"><code># predict to US
pred.r=raster::predict(clim.us,mod.maxnet, index=1,type=&quot;exponential&quot;) # note &#39;type&#39; differs from glm
pred.r=pred.r/sum(values(pred.r),na.rm=T) # normalize prediction (sum to 1)
plot(log(pred.r)) # plot raster
plot(pres,add=T) # plot points</code></pre>
<p><img src="101SDMs_files/figure-html/unnamed-chunk-30-1.png" /><!-- --></p>
</div>
<div id="evaluate-performance-2" class="section level3">
<h3><span class="header-section-number">3.6.2</span> Evaluate performance</h3>
<pre class="r"><code># evaluate
pred.at.fitting.pres=raster::extract(pred.r,pres.data) # get predictions at pres locations
pred.at.fitting.bg=raster::extract(pred.r,bg.data) # get predictions at background locations
rocr.pred=ROCR::prediction(predictions=c(pred.at.fitting.pres,pred.at.fitting.bg),
                          labels=c(rep(1,length(pred.at.fitting.pres)),rep(0,length(pred.at.fitting.bg)))) # define the prediction object needed by ROCR
perf.fit=performance(rocr.pred,measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) # calculate perfomance 
plot(perf.fit) # plot ROC curve
abline(0,1) # 1:1 line indicate random predictions </code></pre>
<p><img src="101SDMs_files/figure-html/unnamed-chunk-31-1.png" /><!-- --></p>
<pre class="r"><code>(auc_ROCR &lt;- performance(rocr.pred, measure = &quot;auc&quot;)@y.values[[1]]) # get AUC</code></pre>
<pre><code>## [1] 0.789735</code></pre>
<p>Great, we’ve recovered some of the predictive accuracy lost when the sampling bias was factored out. By picking up more complex responses, we’re better able to describe the distribution. An important caveat is that we should check for overfitting, wherein we’ve fit to idiosyncracies of the particular fitting data set. To check this, we’d need to evaluate on independent data, which we won’t get to here <em>but is a critical step on any SDM you intend to publish.</em></p>
</div>
</div>
<div id="other-options" class="section level2">
<h2><span class="header-section-number">3.7</span> Other options</h2>
<p>Within the framework described above:</p>
<ul>
<li>Thin presences to remove spatial autocorrelation</li>
<li>Subsample presences to evaluate model on independent data (e.g., k-fold cross validation)</li>
<li>Use remotely sensed data (removing artefacts of interpolation)</li>
<li>More informative performance statistics (Boyce, partial AUC)</li>
<li>Other algorithms (GAMs, Tree-based methods, Envelope methods)</li>
<li>Model selection to find better suites of predictors</li>
</ul>
<p>Somewhat different frameworks:</p>
<ul>
<li>Model space explicitly (accounting for spatial autocorrelation)</li>
<li>Borrow strength from other species (Joint SDMs)</li>
<li>Bayesian models for more complete treatment of uncertainty or hierarchical structure, among other things</li>
<li>Ensemble approaches to combine the results of multiple models</li>
<li>Combine data from different parts of the distribution (e.g. native range) <!-- #========================================================================= --> <!-- ## Thin presences, Stratify sampling --></li>
</ul>
<!-- ```{r} -->
<!-- all.data$weight = all.data$pres + (1 - all.data$pres) * 10000 # these allow you to fit a Point Process -->
<!-- mod.worst=glm(form,data=all.data,family=poisson(link='log'),weights=weight) # fit the model -->
<!-- #mod.worst=maxnet(all.data$pres,all.data[-1]) -->
<!-- summary(mod.worst) # show coefficients -->
<!-- ``` -->
<!-- ## Model Comparison -->
<!-- # # evaluate transfer -->
<!-- # pred.at.transfer.pres=raster::extract(transfer.r,pres.data) -->
<!-- #   # sample background in transfer region -->
<!-- # all.background=which(complete.cases(values(clim.us))) -->
<!-- # bg.index=sample(all.background,10000) -->
<!-- # bg.data=data.frame(values(clim.us)[bg.index,]) -->
<!-- # coordinates(bg.data)=coordinates(clim.us)[bg.index,] -->
<!-- #  -->
<!-- # transfer.bg= -->
<!-- # pred.at.fitting.bg=raster::extract(transfer.r,bg.data) -->
<!-- # rocr.pred=ROCR::prediction(predictions=c(pred.at.fitting.pres,pred.at.fitting.bg), -->
<!-- #                           labels=c(rep(1,length(pred.at.fitting.pres)),rep(0,length(pred.at.fitting.bg)))) -->
<!-- # perf.fit=performance(rocr.pred,measure = "tpr", x.measure = "fpr") -->
<!-- # plot(perf.fit) -->
<!-- # abline(0,1) -->
<!-- # (auc_ROCR <- performance(rocr.pred, measure = "auc")@y.values[[1]]) -->
<!-- #  -->
</div>
</div>


<!-- give the footer some space -->
<br/>
<br/>

<footer id="site-footer">
  <div id="footer1">
  <a href="https://cmerow.github.io"><img src="img/cory.png" alt="logo" width=40px></a>
  <!--
  <a href="http://adamwilson.us/#contact"><i class="fa fa-envelope fa-2x"></i></a> 
  <a href="https://twitter.com/AdamWilsonLab"><i class="fa fa-twitter fa-2x"></i></a> 
  <a href="https://github.com/AdamMWilson"><i class="fa fa-github fa-2x"></i></a>
  -->
  </div>
  <div id="footer2">
  <a rel="license" property="http://creativecommons.org/ns#license"
  href="http://creativecommons.org/licenses/by/4.0/" ><img src="img/cc-by.svg" alt="cc-by"/></a> 
  </div>
</footer>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>


</body>
</html>
